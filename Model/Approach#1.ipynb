{"cells":[{"cell_type":"markdown","metadata":{"id":"LLBAFveEn8yt"},"source":["- reference: https://github.com/dh1105/Sentence-Entailment/blob/main/Sentence_Entailment_BERT.ipynb"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ZyEyJZDkVV5","executionInfo":{"status":"ok","timestamp":1670195616352,"user_tz":480,"elapsed":11423,"user":{"displayName":"Nan Qiang","userId":"07345378033215295339"}},"outputId":"12c497b0-41bf-487d-93d9-fd8bb243e61f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 24.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 75.5 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 70.8 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":40024,"status":"ok","timestamp":1670195656374,"user":{"displayName":"Nan Qiang","userId":"07345378033215295339"},"user_tz":480},"id":"zCOlSRB59cuO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca596b64-820d-4bb7-b41d-dd3b4b1a117c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler\n","from torch.nn.utils.rnn import pad_sequence\n","import pickle\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":334,"status":"ok","timestamp":1670195756045,"user":{"displayName":"Nan Qiang","userId":"07345378033215295339"},"user_tz":480},"id":"U1MoE7_SkkIi"},"outputs":[],"source":["# change the model, 1 for deberta, 0 for bert\n","choice = 1\n","# whether or not to apply data cleaning action, 1 for apply, 0 for not apply\n","cleaning_choice = 1\n","# change path data where the data files\n","path_data = '/content/drive/My Drive/multi-evidence-nli-nlp-243-master/Data/'"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1670195756423,"user":{"displayName":"Nan Qiang","userId":"07345378033215295339"},"user_tz":480},"id":"w-DT8T-bk8WS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b03f0b86-198b-4fd6-b0c7-e12530badd71"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["# set the deviceto gpu if it exist\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"xc3uWvWIkVV-","executionInfo":{"status":"ok","timestamp":1670195756762,"user_tz":480,"elapsed":1,"user":{"displayName":"Nan Qiang","userId":"07345378033215295339"}}},"outputs":[],"source":["import json\n","# get train.json and force it into single data type\n","f = open(path_data + 'train.json') \n","data = json.load(f)\n","f.close()\n","df=pd.DataFrame.from_dict(data, orient='index')\n","df = df[df.Type == \"Single\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P37SR2ifkVV_"},"outputs":[],"source":["\n","# traverse the CTR file to get all the CTRs\n","dir_path = path_data + 'CT json'\n","all_json_files = [join(dir_path, f) for f in listdir(dir_path) if isfile(join(dir_path, f)) and f.endswith(\".json\")]\n","\n","ctr_list = []\n","for file_path in all_json_files:\n","    with open(file_path) as input_file:\n","        json_array = json.load(input_file)\n","        ctr_list.append(json_array)\n","ctr_df = pd.json_normalize(ctr_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiea-2FskVWA"},"outputs":[],"source":["#get test data by \n","def get_train_val_test_data_split(df):\n","    test_ind = int(len(df)*0.85)\n","    val_ind = int(len(df)*0.7)            \n","        \n","    return df[:val_ind], df[val_ind:test_ind], df[test_ind:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qarsxWD5kVWA"},"outputs":[],"source":["df.rename(columns = {'Label':'label'}, inplace = True)\n","# split the data into training, validation, and test dataframe\n","train_df, val_df, test_df = get_train_val_test_data_split(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uA2w76ZKkVWB"},"outputs":[],"source":["# skip cleaning process if the choice is set to 0\n","if cleaning_choice == 0:\n","    def processing(df, ctr_df):\n","        # get all information neccessart for constructing data\n","        p_id = df['Primary_id']\n","        s_id = df['Section_id']\n","        labels = df['label']\n","        state = df['Statement']\n","        p_index = df['Primary_evidence_index']\n","        check_lst = [\"intervention\", \"disease characteristics\", \"patient characteristics\", \"results\", \"adverse events\"]\n","        ctr, hypothesis, label = [], [], []\n","        for p, s, l, st, p_i in zip(p_id, s_id, labels, state, p_index):\n","            # retreive the CTR when clinical trial ID is matched\n","            temp = ctr_df.loc[ctr_df['Clinical Trial ID'] == p].drop('Clinical Trial ID', inplace=False, axis=1)\n","            for lst in temp:\n","                app_str = \"\"\n","                for premises in temp[lst]:\n","                    for i, sen in enumerate(premises):\n","                        # get the premise and hypothesis\n","                        ctr.append(sen)\n","                        hypothesis.append(st)\n","                        # check if the sentence is non-neutral label\n","                        if s == lst and i in p_i:\n","                            label.append(l.lower())\n","                        else:\n","                            label.append(\"neutral\")\n","\n","\n","        d = {'ctr': ctr, 'hypothesis':hypothesis, 'label': label}\n","        return pd.DataFrame(d)\n","\n","    train_df = processing(train_df, ctr_df)\n","    val_df = processing(val_df, ctr_df)\n","    test_df = processing(test_df, ctr_df)\n","    p_id = df['Primary_id']\n","    \n","# doing cleaning process if the choice is set to 1    \n","else:  \n","    def processing(df, ctr_df):\n","        # get all information neccessart for constructing data\n","        p_id = df['Primary_id']\n","        s_id = df['Section_id']\n","        labels = df['label']\n","        state = df['Statement']\n","        p_index = df['Primary_evidence_index']\n","        ctr, hypothesis, label = [], [], []\n","        for p, s, l, st, p_i in zip(p_id, s_id, labels, state, p_index):\n","            # retreive the CTR when clinical trial ID is matched\n","            temp = ctr_df.loc[ctr_df['Clinical Trial ID'] == p].drop('Clinical Trial ID', inplace=False, axis=1)\n","            for lst in temp:\n","                app_str = \"\"\n","                for premises in temp[lst]:\n","                    for i, sen in enumerate(premises):\n","                        if sen.strip().endswith(\":\"):\n","                            app_str+=\" \"+sen\n","                            continue\n","                        elif sen != []:\n","                            ctr.append(app_str+sen)\n","                            hypothesis.append(st)\n","                            # check if the sentence is non-neutral label\n","                            if s == lst and i in p_i:\n","                                label.append(l.lower())\n","                            else:\n","                                label.append(\"neutral\")\n","                            app_str = \"\"\n","        d = {'ctr': ctr, 'hypothesis':hypothesis, 'label': label}\n","        return pd.DataFrame(d)\n","    \n","    train_df = processing(train_df, ctr_df)\n","    val_df = processing(val_df, ctr_df)\n","    test_df = processing(test_df, ctr_df)\n","    p_id = df['Primary_id']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwsQVx1NkVWC","outputId":"df63e7b0-3742-4da0-ac93-1c5604bf6e37"},"outputs":[{"name":"stdout","output_type":"stream","text":["neutral          34203\n","contradiction     2958\n","entailment        2883\n","Name: label, dtype: int64\n","neutral          7307\n","contradiction     639\n","entailment        565\n","Name: label, dtype: int64\n","neutral          7972\n","entailment        599\n","contradiction     535\n","Name: label, dtype: int64\n"]}],"source":["print(train_df['label'].value_counts())\n","print(val_df['label'].value_counts())\n","print(test_df['label'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnSh_6c7kVWC","outputId":"c73347c0-887e-4012-db11-193de5390b9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["2883\n"]}],"source":["# under sample to get same number of labels\n","random_state = 42\n","train_min = min(train_df['label'].value_counts())\n","print(train_min)\n","s0 = train_df.label[train_df.label.eq(\"neutral\")].sample(train_min, random_state=random_state).index\n","s1 = train_df.label[train_df.label.eq(\"contradiction\")].sample(train_min, random_state=random_state).index \n","s2 = train_df.label[train_df.label.eq(\"entailment\")].sample(train_min, random_state=random_state).index \n","train_df = train_df.loc[s0.union(s1).union(s2)]\n","\n","val_min = min(val_df['label'].value_counts())\n","s0 = val_df.label[val_df.label.eq(\"neutral\")].sample(val_min, random_state=random_state).index\n","s1 = val_df.label[val_df.label.eq(\"contradiction\")].sample(val_min, random_state=random_state).index \n","s2 = val_df.label[val_df.label.eq(\"entailment\")].sample(val_min, random_state=random_state).index \n","val_df = val_df.loc[s0.union(s1).union(s2)]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1669322681908,"user":{"displayName":"Nan Qiang","userId":"07345378033215295339"},"user_tz":480},"id":"nu2vYRLxlQjf","outputId":"a92c7b5b-47c2-46c8-e94d-2945a18e9e5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["neutral          2883\n","contradiction    2883\n","entailment       2883\n","Name: label, dtype: int64\n","neutral          565\n","entailment       565\n","contradiction    565\n","Name: label, dtype: int64\n","neutral          7972\n","entailment        599\n","contradiction     535\n","Name: label, dtype: int64\n"]}],"source":["print(train_df['label'].value_counts())\n","print(val_df['label'].value_counts())\n","print(test_df['label'].value_counts())"]},{"cell_type":"markdown","metadata":{"id":"sdbUBwu7-d4C"},"source":["#### Prepare dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnIuhH9Elffi"},"outputs":[],"source":["class MNLIDataBert(Dataset):\n","    # initialize the dataset\n","    def __init__(self, train_df, val_df, test_df):\n","        self.label_dict = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n","\n","        self.train_df = train_df\n","        self.val_df = val_df\n","        self.test_df = test_df\n","        # if choice is 0, choose bert, otherwise choose deberta\n","        if choice == 0:\n","            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","        else:\n","            self.tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-base')\n","        self.train_data = None\n","        self.val_data = None\n","        self.test_data = None\n","        self.init_data()\n","    \n","    # load data splits\n","    def init_data(self):\n","        self.train_data = self.load_data(self.train_df)\n","        self.val_data = self.load_data(self.val_df)\n","        self.test_data = self.load_data(self.test_df)\n","\n","    def load_data(self, df):\n","        MAX_LEN = 512\n","        token_ids = []\n","        mask_ids = []\n","        seg_ids = []\n","        y = []\n","\n","        premise_list = df['ctr'].to_list()\n","        hypothesis_list = df['hypothesis'].to_list()\n","        label_list = df['label'].to_list()\n","\n","        for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n","            # tokenize the premise and the hypothesis\n","            premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n","            hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n","            # apply cls and sep token to the premise and hypothesis\n","            pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n","            premise_len = len(premise_id)\n","            hypothesis_len = len(hypothesis_id)\n","            \n","            # get the mask id\n","            segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n","            attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n","            \n","            token_ids.append(torch.tensor(pair_token_ids))\n","            seg_ids.append(segment_ids)\n","            mask_ids.append(attention_mask_ids)\n","            y.append(self.label_dict[label])\n","        \n","        # pad the sequence for the dataset\n","        token_ids = pad_sequence(token_ids, batch_first=True)\n","        mask_ids = pad_sequence(mask_ids, batch_first=True)\n","        seg_ids = pad_sequence(seg_ids, batch_first=True)\n","        y = torch.tensor(y)\n","        dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n","        return dataset\n","\n","    # get dataloaders for data splits\n","    def get_data_loaders(self, batch_size=32, shuffle=True):\n","        train_loader = DataLoader(\n","          self.train_data,\n","          shuffle=shuffle,\n","          batch_size=batch_size\n","        )\n","\n","        val_loader = DataLoader(\n","          self.val_data,\n","          shuffle=shuffle,\n","          batch_size=batch_size\n","        )\n","\n","        test_loader = DataLoader(\n","          self.test_data,\n","          shuffle=False,\n","          batch_size=batch_size\n","        )\n","\n","        return train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnHCTjUjmOBo"},"outputs":[],"source":["mnli_dataset = MNLIDataBert(train_df, val_df, test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfEadOkMnXZb"},"outputs":[],"source":["train_loader, val_loader, test_loader = mnli_dataset.get_data_loaders(batch_size=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":39992,"status":"aborted","timestamp":1669322681912,"user":{"displayName":"Nan Qiang","userId":"07345378033215295339"},"user_tz":480},"id":"CC6DZvGjnbYG","outputId":"3fd3382d-0b91-4e7a-80fd-6fd6c7793ce0"},"outputs":[{"data":{"text/plain":["DebertaForSequenceClassification(\n","  (deberta): DebertaModel(\n","    (embeddings): DebertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n","      (LayerNorm): DebertaLayerNorm()\n","      (dropout): StableDropout()\n","    )\n","    (encoder): DebertaEncoder(\n","      (layer): ModuleList(\n","        (0): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (1): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (2): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (3): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (4): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (5): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (6): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (7): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (8): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (9): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (10): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (11): DebertaLayer(\n","          (attention): DebertaAttention(\n","            (self): DisentangledSelfAttention(\n","              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n","              (pos_dropout): StableDropout()\n","              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n","              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): DebertaLayerNorm()\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): DebertaLayerNorm()\n","            (dropout): StableDropout()\n","          )\n","        )\n","      )\n","      (rel_embeddings): Embedding(1024, 768)\n","    )\n","  )\n","  (pooler): ContextPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): StableDropout()\n","  )\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n","  (dropout): StableDropout()\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["\n","# if choice is 0, choose bert, otherwise choose deberta\n","if choice == 0:\n","    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n","else:\n","    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-base')\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-R_fmf6nqU3"},"outputs":[],"source":["param_optimizer = list(model.named_parameters())\n","# set weight decay value against bias, gamma, and beta\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.05},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stBuiMv2nrlE"},"outputs":[],"source":["import torch\n","# This variable contains all of the hyperparemeter information our training loop needs\n","optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=2e-6)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eM7myNknvLS"},"outputs":[],"source":["# calculating the accuracy for multi-label\n","def multi_acc(y_pred, y_test):\n","    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qx1QDOOgqIKL"},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQPig0Qenw2l"},"outputs":[],"source":["import time\n","# set manual seed to 0 so that the result is reproducible\n","torch.manual_seed(0)\n","# set num of epoch\n","EPOCHS = 2\n","\n","def train(model, train_loader, val_loader, optimizer):  \n","    total_step = len(train_loader)\n","    \n","    for epoch in range(EPOCHS):\n","        start = time.time()\n","        model.train()\n","        total_train_loss = 0\n","        total_train_acc  = 0\n","        # get the training batch\n","        with tqdm(train_loader, unit=\"batch\") as tepoch:\n","            for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(tepoch):\n","                tepoch.set_description(f\"Epoch {epoch}\")\n","                optimizer.zero_grad()\n","                pair_token_ids = pair_token_ids.to(device)\n","                mask_ids = mask_ids.to(device)\n","                seg_ids = seg_ids.to(device)\n","                labels = y.to(device)\n","                \n","                # get prediction logits and loss\n","                loss, prediction = model(pair_token_ids, \n","                                  token_type_ids=seg_ids, \n","                                  attention_mask=mask_ids, \n","                                  labels=labels).values()\n","                # get final predictions\n","                pred = torch.log_softmax(prediction, dim=1).argmax(dim=1)\n","                # get multi label accuracy\n","                acc = multi_acc(prediction, labels)\n","                \n","                loss.backward()\n","                optimizer.step()\n","\n","                total_train_loss += loss.item()\n","                total_train_acc  += acc.item()\n","        # calculate the traning accuracy and loss\n","        train_acc  = total_train_acc/len(train_loader)\n","        train_loss = total_train_loss/len(train_loader)\n","        \n","        model.eval()\n","        total_val_acc  = 0\n","        total_val_loss = 0\n","        \n","        # calculate the traning accuracy and loss\n","        with torch.no_grad():\n","            for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n","                optimizer.zero_grad()\n","                pair_token_ids = pair_token_ids.to(device)\n","                mask_ids = mask_ids.to(device)\n","                seg_ids = seg_ids.to(device)\n","                labels = y.to(device)\n","\n","                # get prediction logits and loss\n","                loss, prediction = model(pair_token_ids, \n","                                    token_type_ids=seg_ids, \n","                                    attention_mask=mask_ids, \n","                                    labels=labels).values()\n","\n","                # get final accuracy\n","                acc = multi_acc(prediction, labels)\n","\n","                total_val_loss += loss.item()\n","                total_val_acc  += acc.item()\n","                \n","        # calculate the traning accuracy and loss\n","        val_acc  = total_val_acc/len(val_loader)\n","        val_loss = total_val_loss/len(val_loader)\n","        end = time.time()\n","        hours, rem = divmod(end-start, 3600)\n","        minutes, seconds = divmod(rem, 60)\n","\n","        print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n","        print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1669322682210,"user":{"displayName":"Nan Qiang","userId":"07345378033215295339"},"user_tz":480},"id":"fP7gi9tJn5pW","scrolled":false,"outputId":"e5563679-c074-4716-8b56-b3f233705c60"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 0:   0%|                                                                             | 0/1082 [00:00<?, ?batch/s]E:\\Anaconda\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n","E:\\Anaconda\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n","Epoch 0: 100%|██████████████████████████████████████████████████████████████████| 1082/1082 [04:01<00:00,  4.48batch/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: train_loss: 1.1042 train_acc: 0.4588 | val_loss: 1.0086 val_acc: 0.4425\n","00:04:16.08\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████████████████████████████████████████████████████████████| 1082/1082 [03:56<00:00,  4.57batch/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: train_loss: 0.7776 train_acc: 0.6415 | val_loss: 1.1270 val_acc: 0.4588\n","00:04:12.35\n"]}],"source":["train(model, train_loader, val_loader, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7YzhHxVkVWH"},"outputs":[],"source":["def get_predictions(model, test_loader, optimizer):\n","    all_predictions = []\n","    all_labels = []\n","    with torch.no_grad():\n","          for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(test_loader):\n","            optimizer.zero_grad()\n","            pair_token_ids = pair_token_ids.to(device)\n","            mask_ids = mask_ids.to(device)\n","            seg_ids = seg_ids.to(device)\n","            labels = y.to(device)\n","            all_labels.append(labels)\n","            # get prediction logits and loss\n","            loss, prediction = model(pair_token_ids, \n","                                token_type_ids=seg_ids, \n","                                attention_mask=mask_ids, \n","                                labels=labels).values()\n","            # get final predictions\n","            pred = torch.log_softmax(prediction, dim=1).argmax(dim=1).float()\n","            # save the predictions\n","            all_predictions.append(pred)\n","            \n","    return all_predictions, all_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VId7buXmkVWH"},"outputs":[],"source":["all_test_predictions, all_test_labels = get_predictions(model, test_loader, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIw-MFhakVWH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUG0QhonkVWI"},"outputs":[],"source":["# flatten the returned label\n","test_labels = [x.tolist() for x in all_test_labels]\n","test_labels = [j for sub in test_labels for j in sub]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vq9wiCxmkVWI","outputId":"cf9d95d5-3f90-4ba7-ce32-98acadde15aa"},"outputs":[{"data":{"text/plain":["1139"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["len(all_test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yWlLotZkVWI"},"outputs":[],"source":["# flatten the predictions\n","test_predictions = [x.tolist() for x in all_test_predictions]\n","test_predictions = [j for sub in test_predictions for j in sub]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXExGRrykVWI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9AaB0C-gkVWJ"},"outputs":[],"source":["# regroup the final test\n","def get_final_ans(df, pred):\n","    idx = 0\n","    grouped_ans = []\n","    group = []\n","    while idx < len(df)-1:\n","        if df[idx:idx+1]['hypothesis'].item() == df[idx+1:idx+2]['hypothesis'].item():\n","            group.append(pred[idx])\n","        else:\n","            group.append(pred[idx])\n","            grouped_ans.append(group)\n","            group = []\n","        idx+=1\n","    return grouped_ans\n","\n","final_grouping = get_final_ans(test_df, test_predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUIY-9MwkVWJ"},"outputs":[],"source":["import collections\n","import random\n","random.seed(0)\n","# get final prediction by grouping\n","def get_final_pred(grouping):\n","    ans = []\n","    ignore = {2.0}\n","    for i, g in enumerate(grouping):\n","        frequency = collections.Counter(x for x in g if x not in ignore).most_common(1)\n","        # if the model predict all neutral, we have to randomly \n","        if len(frequency) == 0:\n","            frequency = random.randint(0,1)\n","        else:\n","            frequency = frequency[0][0]\n","        ans.append(frequency)\n","    return ans\n","prediction = get_final_pred(final_grouping)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"800O8CK_kVWJ"},"outputs":[],"source":["# match hypothesis into a group\n","def get_hypothesis(df):\n","    idx = 0\n","    grouped_ans = []\n","    group = []\n","    while idx < len(df)-1:\n","        # if they are the same hypothesis, get them back into one\n","        if df[idx:idx+1]['hypothesis'].item() == df[idx+1:idx+2]['hypothesis'].item():\n","            idx+=1\n","            continue\n","        else:\n","            group.append(df[idx:idx+1]['hypothesis'].item())\n","        idx+=1\n","    return group"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"in38zPfGkVWK"},"outputs":[],"source":["h_lst = get_hypothesis(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"aWztvTv9kVWK","outputId":"c85d6056-1e2b-451f-8a26-8f7b642825a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.48      0.29      0.36        84\n","           1       0.43      0.63      0.51        71\n","\n","    accuracy                           0.45       155\n","   macro avg       0.45      0.46      0.43       155\n","weighted avg       0.46      0.45      0.43       155\n","\n"]}],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","label_dict = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n","\n","true = []\n","for h in h_lst:\n","    true.append(label_dict[df[df['Statement'] == h]['label'].tolist()[0].lower()])\n","\n","\n","print(classification_report(true, prediction))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vu-ctBXVkVWK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7rjIZhGkVWL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8I8b3-zkVWL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCyGuhBykVWL"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1Y1_3IatdFWGDm4mBGhszeCZVhdX8Yr2L","timestamp":1669322135313},{"file_id":"1_FGfNi68bJanGnhHltCG8md_lBvLH6J6","timestamp":1669191315918}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}